import tensorflow as tf
from tensorflow.keras.applications import VGG16
from tensorflow.keras.layers import Dense, Flatten, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam

num_classes = 10

print("Loading pre-trained VGG16 model...")
base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
print("Base model summary:")
base_model.summary()

for layer in base_model.layers:
    layer.trainable = False
print("Convolutional layers frozen.")

x = Flatten()(base_model.output)
x = Dense(512, activation='relu')(x)
x = Dropout(0.5)(x)
x = Dense(256, activation='relu')(x)
output_layer = Dense(num_classes, activation='softmax')(x)

model = Model(inputs=base_model.input, outputs=output_layer)
print("\nFinal model architecture with custom classification layers added:")
model.summary()

model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])
print("\nModel compiled with Adam optimizer and categorical crossentropy loss.")

print("\nTraining the model on the custom classifier layers only...")

for layer in base_model.layers[-4:]:
    layer.trainable = True
print("\nUnfroze the last 4 layers of the base model for fine-tuning.")

model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])
print("\nModel recompiled with a lower learning rate for fine-tuning.")

print("\nFine-tuning the model...")
